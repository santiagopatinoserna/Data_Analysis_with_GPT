{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lisis de Datos con GPT + Data Science üöÄ\n",
    "\n",
    "## ¬øQu√© Haremos? \n",
    "Exploraremos c√≥mo GPT revoluciona el an√°lisis de datos en Python durante una sesi√≥n de 40 minutos. Veremos un flujo de trabajo moderno que combina t√©cnicas tradicionales con IA.\n",
    "\n",
    "## Objetivos üéØ\n",
    "\n",
    "1. **Data Science + GPT**\n",
    "   - Proceso cl√°sico mejorado con IA\n",
    "   - Casos pr√°cticos reales\n",
    "\n",
    "2. **Toma de Decisiones**\n",
    "   - Decisiones basadas en datos + GPT\n",
    "   - Validaci√≥n de insights\n",
    "\n",
    "3. **Automatizaci√≥n**\n",
    "   - Uso eficiente de la API\n",
    "   - Tareas rutinarias optimizadas\n",
    "\n",
    "4. **Interpretaci√≥n**\n",
    "   - An√°lisis asistido por IA\n",
    "   - Insights accionables\n",
    "\n",
    "‚Üí Enfoque paso a paso con ejemplos reales üìà\n",
    "\n",
    "*Nota: Experiencia pr√°ctica con c√≥digo y resultados tangibles*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Inicial üõ†Ô∏è\n",
    "\n",
    "Configuramos nuestro ambiente y cargamos datos para an√°lisis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Configuramos la API de OpenAI\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"api_key_gpt\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "print(\"Bibliotecas importadas y API de OpenAI configurada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚Üí Dataset listo para an√°lisis con GPT üìä\n",
    "\n",
    "*Tip: Verifica tu API key en .env*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Configuraci√≥n de API GPT-4 ü§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_query(prompt):\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Eres un experto en an√°lisis de datos y machine learning, capaz de proporcionar insights profundos y recomendaciones pr√°cticas.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error al interactuar con GPT: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚Üí Funci√≥n lista para consultas de an√°lisis üîç\n",
    "\n",
    "*Tip: Mant√©n los prompts concisos y espec√≠ficos*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Carga y Exploraci√≥n Inicial üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lectura de los datos \n",
    "california_housing = fetch_california_housing()\n",
    "data = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)\n",
    "data['MedHouseVal'] = california_housing.target\n",
    "\n",
    "print(\"Dataset cargado. Primeras filas:\")\n",
    "display(data.head())\n",
    "\n",
    "print(\"\\nInformaci√≥n del dataset:\")\n",
    "display(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. An√°lisis Exploratorio con GPT üîç\n",
    "\n",
    "üéØ Objetivos del an√°lisis:\n",
    "- Detectar patrones clave\n",
    "- Identificar correlaciones importantes\n",
    "- Obtener insights para preprocesamiento\n",
    "\n",
    "*Tip: GPT complementar√° nuestro an√°lisis t√©cnico con insights adicionales* üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos GPT para generar recomendaciones sorprendentes, avanzadas y f√°ciles de entender para un data scientist\n",
    "\n",
    "initial_recommendations = gpt_query(f\"\"\"\n",
    "Este es un dataset de viviendas en California. Aqu√≠ tienes un resumen detallado de la informaci√≥n del dataset:\n",
    "\n",
    "1. N√∫mero de filas y columnas: {data.shape}\n",
    "2. Caracter√≠sticas: {data.columns.tolist()}\n",
    "3. Valores nulos en las columnas: {data.isnull().sum().to_string()}\n",
    "4. Descripci√≥n estad√≠stica del dataset:\n",
    "{data.describe().to_string()}\n",
    "5. Correlaciones entre caracter√≠sticas:\n",
    "{data.corr().to_string()}\n",
    "\n",
    "Con base en esta informaci√≥n, proporciona recomendaciones avanzadas que un data scientist deber√≠a seguir para optimizar este dataset de cara al modelado. Incluye solo acciones claras y f√°ciles de entender:\n",
    "\n",
    "Solo responde con recomendaciones claras, pr√°cticas y de alto nivel que sean sorprendentes y √∫tiles para un data scientist experimentado.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚Üì Veremos:\n",
    "- Insights clave del modelo\n",
    "- Recomendaciones pr√°cticas\n",
    "- Pr√≥ximos pasos sugeridos\n",
    "\n",
    "*Tip: Presta atenci√≥n a los patrones no obvios identificados por GPT* üîç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.markdown import Markdown\n",
    "console = Console()\n",
    "\n",
    "console.print(Markdown(initial_recommendations))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 An√°lisis de Correlaciones üìä\n",
    "\n",
    "üîç Observamos:\n",
    "- Correlaciones importantes con MedHouseVal\n",
    "- Posibles variables redundantes\n",
    "- Patrones para feature selection\n",
    "\n",
    "*Tip: Los colores m√°s intensos indican correlaciones m√°s fuertes* üé®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la matriz de correlaci√≥n\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "# Configurar el tama√±o del gr√°fico\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Crear un heatmap para mostrar la matriz de correlaci√≥n\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt=\".2f\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Insights de Correlaci√≥n con GPT üéØ\n",
    "\n",
    "\n",
    "üîç Enfoque en:\n",
    "- Variables m√°s predictivas\n",
    "- Multicolinealidad\n",
    "- Estrategias de feature selection\n",
    "\n",
    "*Nota: GPT identificar√° patrones no evidentes en las correlaciones* üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos GPT para generar recomendaciones sorprendentes, avanzadas y f√°ciles de entender para un data scientist\n",
    "\n",
    "# Variable a predecir \n",
    "target_var = 'MedHouseVal'\n",
    "\n",
    "\n",
    "Correlation_recomendation = gpt_query(f\"\"\"\n",
    "Analiza la siguiente matriz de correlaci√≥n y proporciona recomendaciones espec√≠ficas, concisas,  para la toma de decisiones como data scientist en la construcci√≥n de un modelo predictivo. El objetivo es es predecir la variable {target_var}:\n",
    "{corr_matrix.to_string()}\n",
    "\"\"\")\n",
    "\n",
    "# Mostrar el panel con las recomendaciones\n",
    "console.print(Markdown(Correlation_recomendation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Feature Scaling üìè\n",
    "\n",
    "Las variables tienen escalas muy diferentes:\n",
    "- Population: 3 - 35,682 \n",
    "- AveRooms: 0.8 - 141.9\n",
    "\n",
    "‚ö†Ô∏è Esto puede afectar:\n",
    "- Convergencia del modelo\n",
    "- Interpretabilidad\n",
    "- Performance general\n",
    "\n",
    "*Nota: Implementaremos escalado durante el modelado* ‚û°Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Feature Engineering üî®\n",
    "\n",
    "Ideas clave:\n",
    "- Distancias a puntos clave\n",
    "- Ratios y combinaciones\n",
    "- Transformaciones geogr√°ficas\n",
    "\n",
    "*Pr√≥ximo paso: Implementar features sugeridos* üó∫Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Generaci√≥n de Features con GPT üõ†Ô∏è\n",
    "Buscaremos:\n",
    "- Interacciones entre variables\n",
    "- Transformaciones √∫tiles\n",
    "- Features geogr√°ficos\n",
    "\n",
    "*GPT combinar√° insights previos para sugerir features √≥ptimos* üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable a predecir \n",
    "target_var = 'MedHouseVal'\n",
    "\n",
    "Feature_eng_reco = gpt_query(f\"\"\"\n",
    "Ten en cuenta las siguientes recomendaciones que me diste, y ay√∫dame a generar ideas para crear nuevas variables para realizar un modelo predictivo de la variable {target_var}:\n",
    "{Correlation_recomendation}, {initial_recommendations}.\n",
    "\"\"\")\n",
    "console.print(Markdown(Feature_eng_reco))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Implementaci√≥n de Features üîß\n",
    "Features creados:\n",
    "- Interacciones de variables üìä\n",
    "- Distancias geogr√°ficas üó∫Ô∏è\n",
    "- Transformaciones logar√≠tmicas üìà\n",
    "\n",
    "*Tip: Validaremos su utilidad en el modelado* üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_distancia(lat1, lon1, lat2=34.05, lon2=-118.25):  # Lat/Lon de Los Angeles como ejemplo\n",
    "    # Convertir grados en radianes\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Diferencias\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    # F√≥rmula de Haversine\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    # Radio de la Tierra en kil√≥metros: aproximadamente 6371\n",
    "    km = 6371 * c\n",
    "    return km\n",
    "\n",
    "# Puedes ajustar lat2 y lon2 a la ubicaci√≥n que consideres referencia, como una ciudad principal\n",
    "def calcular_distancia_costa(lat, lon):\n",
    "    # Supongamos que (34.05, -118.25) es un punto en la costa de California\n",
    "    return calcular_distancia(lat, lon, 34.05, -118.25)  # Usa latitudes y longitudes reales para tu caso\n",
    "\n",
    "\n",
    "data['MedInc_por_HouseAge'] = data['MedInc'] * data['HouseAge']\n",
    "data['MedInc_por_AveRooms'] = data['MedInc'] * data['AveRooms']\n",
    "data['HouseAge_por_AveRooms'] = data['HouseAge'] * data['AveRooms']\n",
    "data['Inc_per_Room'] = data['MedInc'] / data['AveRooms']\n",
    "data['Distancia_Principalidad'] = data.apply(lambda row: calcular_distancia(row['Latitude'], row['Longitude']), axis=1) # Necesitar√°s una funci√≥n para calcular distancias\n",
    "data['Distancia_Costa'] = data.apply(lambda row: calcular_distancia_costa(row['Latitude'], row['Longitude']), axis=1) # Similar a lo anterior, necesitas una funci√≥n espec√≠fica\n",
    "data['Poblaci√≥n_Normalizada'] = data['Population'] / data['Population'].max()  # Normalizaci√≥n simple\n",
    "data['MedInc_Log'] = np.log(data['MedInc'])\n",
    "data['HouseAge_squared'] = data['HouseAge'] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 An√°lisis de Outliers üîç\n",
    "\n",
    "Esta funci√≥n identifica valores at√≠picos usando el m√©todo IQR (Rango Intercuartil):\n",
    "\n",
    "üìä Nos dir√°:\n",
    "- Cu√°ntos outliers hay por variable\n",
    "- Qu√© porcentaje representan\n",
    "- D√≥nde debemos prestar atenci√≥n\n",
    "\n",
    "*Tip: Los l√≠mites ¬±1.5*IQR son el est√°ndar para detecci√≥n* üìà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_outlier_limits_and_counts(df):\n",
    "    outlier_summary = []\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype in ['float64', 'int64']:  # Solo consideramos columnas num√©ricas\n",
    "            p75 = df[column].quantile(0.75)\n",
    "            p25 = df[column].quantile(0.25)\n",
    "            iqr = p75 - p25\n",
    "            upper_limit = p75 + 1.5 * iqr\n",
    "            lower_limit = p25 - 1.5 * iqr\n",
    "\n",
    "            # Contar los outliers\n",
    "            num_outliers = df[(df[column] < lower_limit) | (df[column] > upper_limit)].shape[0]\n",
    "\n",
    "            # Agregar los resultados a la lista\n",
    "            outlier_summary.append({\n",
    "                'Column': column,\n",
    "                'Lower Limit': lower_limit,\n",
    "                'Upper Limit': upper_limit,\n",
    "                'Number of Outliers': num_outliers\n",
    "            })\n",
    "\n",
    "    # Convertir la lista a DataFrame\n",
    "    outlier_df = pd.DataFrame(outlier_summary)\n",
    "    return outlier_df\n",
    "\n",
    "# Calcula los l√≠mites y el conteo de outliers\n",
    "outlier_df = calculate_outlier_limits_and_counts(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Estrategia de Outliers con GPT üéØ\n",
    "\n",
    "Evaluaremos:\n",
    "- Qu√© outliers tratar\n",
    "- M√©todos de tratamiento\n",
    "- Impacto en el modelo\n",
    "\n",
    "*Tip: Balance entre limpieza y preservaci√≥n de informaci√≥n* ‚öñÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos GPT para generar recomendaciones avanzadas, pr√°cticas y claras para cient√≠ficos de datos.\n",
    "\n",
    "# Variable a predecir\n",
    "target_var = 'MedHouseVal'\n",
    "\n",
    "# Generar recomendaciones sobre manejo de outliers\n",
    "outlier_reco = gpt_query(f\"\"\"\n",
    "Considerando el resumen estad√≠stico de los datos: {data.describe().to_string()}, y la informaci√≥n sobre outliers: {outlier_df.to_string()}.\n",
    "Por favor, proporciona recomendaciones breves y directas sobre c√≥mo limpiar los outliers para mejorar la predicci√≥n de la variable '{target_var}'. Aseg√∫rate de que las sugerencias sean espec√≠ficas y tengan sentido. \n",
    "\"\"\")\n",
    "\n",
    "console.print(Markdown(outlier_reco))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.2 Tratamiento de Outliers üîß\n",
    "\n",
    "üîß M√©todos aplicados:\n",
    "- Log transform\n",
    "- Clipping\n",
    "- Normalizaci√≥n\n",
    "\n",
    "*Tip: Verificar impacto en distribuciones* üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Supongamos que `data` es tu DataFrame y ya est√° cargado con los datos\n",
    "\n",
    "### 1. Aplicar Transformaci√≥n Logar√≠tmica a MedInc\n",
    "data['MedInc_Log'] = np.log1p(data['MedInc'])\n",
    "\n",
    "### 2. Imponer L√≠mites a AveRooms\n",
    "data['AveRooms'] = np.clip(data['AveRooms'], 2.02, 8.46)\n",
    "\n",
    "### 3. Establecer L√≠mite Superior para AveBedrms\n",
    "data['AveBedrms'] = np.where(data['AveBedrms'] > 1.24, 1.24, data['AveBedrms'])\n",
    "\n",
    "### 4. Aplicar Estandarizaci√≥n o Transformaci√≥n Logar√≠tmica a Population\n",
    "data['Population_Log'] = np.log1p(data['Population'])\n",
    "\n",
    "### 5. Imponer L√≠mites a AveOccup\n",
    "data['AveOccup'] = np.clip(data['AveOccup'], 1.15, 4.56)\n",
    "\n",
    "### 6. Establecer L√≠mite Superior para MedHouseVal\n",
    "data['MedHouseVal'] = np.where(data['MedHouseVal'] > 4.82, 4.82, data['MedHouseVal'])\n",
    "\n",
    "### 7. Establecer L√≠mite Superior para MedInc_por_HouseAge\n",
    "data['MedInc_por_HouseAge'] = np.where(data['MedInc_por_HouseAge'] > 263.27, 263.27, data['MedInc_por_HouseAge'])\n",
    "\n",
    "### 8. Establecer L√≠mites para MedInc_por_AveRooms\n",
    "data['MedInc_por_AveRooms'] = np.clip(data['MedInc_por_AveRooms'], -12.58, 52.13)\n",
    "\n",
    "### 9. Establecer L√≠mites para HouseAge_por_AveRooms\n",
    "data['HouseAge_por_AveRooms'] = np.clip(data['HouseAge_por_AveRooms'], -50.62, 341.75)\n",
    "\n",
    "### 10. Aplicar Transformaci√≥n Logar√≠tmica o Estandarizaci√≥n a Inc_per_Room\n",
    "data['Inc_per_Room_Log'] = np.log1p(data['Inc_per_Room'])  # Considerando logar√≠tmica si los valores son siempre positivos\n",
    "\n",
    "### 11. Estandarizar la Poblaci√≥n o Aplicar Transformaci√≥n Logar√≠tmica\n",
    "data['Poblaci√≥n_Normalizada'] = (data['Population'] - data['Population'].mean()) / data['Population'].std()\n",
    "\n",
    "### 12. Descartar Outliers en MedInc o Reemplazarlos\n",
    "# Primero calcular los l√≠mites para outliers basados en cuantiles\n",
    "lower_bound, upper_bound = data['MedInc'].quantile([0.01, 0.99])\n",
    "# Reemplazar outliers con la mediana\n",
    "data['MedInc'] = np.where((data['MedInc'] < lower_bound) | (data['MedInc'] > upper_bound), data['MedInc'].median(), data['MedInc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Selecci√≥n de Modelo con GPT ü§ñ\n",
    "\n",
    "üéØ Criterios:\n",
    "- Distribuci√≥n de datos\n",
    "- Relaciones entre variables\n",
    "- Complejidad necesaria\n",
    "\n",
    "*Tip: La recomendaci√≥n se basar√° en caracter√≠sticas del dataset* üìä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Selecci√≥n Inteligente del Modelo con GPT y TOOLSü§ñ\n",
    "\n",
    "\n",
    "üéØ Proceso:\n",
    "- An√°lisis de datos\n",
    "- Selecci√≥n modelo √≥ptimo\n",
    "- Justificaci√≥n t√©cnica\n",
    "\n",
    "*Tip: GPT considera complejidad y caracter√≠sticas del dataset* üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "def get_model_recommendation(data: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Consulta a GPT para obtener recomendaci√≥n del mejor modelo de ML.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame con los datos para analizar\n",
    "        \n",
    "    Returns:\n",
    "        Dict con el modelo recomendado y su justificaci√≥n\n",
    "    \"\"\"\n",
    "    # Preparar los datos para GPT\n",
    "    sample_str = data.sample(20).to_string(index=False)\n",
    "    stats_str = data.describe().to_string()\n",
    "    corr_str = data.corr().to_string()\n",
    "    \n",
    "    \n",
    "    # Definir la funci√≥n tool\n",
    "    tools = [{\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"recommend_model\",\n",
    "            \"description\": \"Recomienda el mejor modelo de ML entre XGBoost, Random Forest y Linear Regression\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"best_model\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"XGBoost\", \"Random Forest\", \"Linear Regression\"],\n",
    "                        \"description\": \"El modelo m√°s apropiado para los datos\"\n",
    "                    },\n",
    "                    \"justification\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Explicaci√≥n de por qu√© se eligi√≥ este modelo\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"best_model\", \"justification\"]\n",
    "            }\n",
    "        }\n",
    "    }]\n",
    "    \n",
    "    try:\n",
    "        # Realizar llamada a OpenAI\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"Eres un experto data scientist. Debes analizar los datos \n",
    "                    proporcionados y recomendar el mejor modelo de ML entre XGBoost, Random Forest \n",
    "                    y Linear Regression. Basa tu decisi√≥n en las caracter√≠sticas de los datos, \n",
    "                    correlaciones y estad√≠sticas descriptivas.\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Por favor analiza estos datos y recomienda el mejor modelo:\n",
    "                    \n",
    "                    Muestra de datos:\n",
    "                    {sample_str}\n",
    "                    \n",
    "                    Estad√≠sticas descriptivas:\n",
    "                    {stats_str}\n",
    "                    \n",
    "                    Matriz de correlaci√≥n:\n",
    "                    {corr_str}\"\"\"\n",
    "                }\n",
    "            ],\n",
    "            tools=tools,\n",
    "            tool_choice={\"type\": \"function\", \"function\": {\"name\": \"recommend_model\"}},\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Extraer la recomendaci√≥n\n",
    "        tool_call = response.choices[0].message.tool_calls[0]\n",
    "        recommendation = eval(tool_call.function.arguments)\n",
    "        \n",
    "        return recommendation\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"best_model\": \"Error\",\n",
    "            \"justification\": f\"Error al obtener recomendaci√≥n: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Evaluaci√≥n del Modelo Recomendado üéØ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_recommendation(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ GPT sugiere Random Forest por:\n",
    "- Manejo de no-linealidad\n",
    "- Robustez a outliers\n",
    "- Captura de interacciones complejas\n",
    "\n",
    "‚û°Ô∏è Pr√≥ximo paso: Implementaci√≥n y validaci√≥n del modelo\n",
    "\n",
    "*Tip: Random Forest funciona bien con nuestras features ingenieradas* üå≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Entrenamiento del Modelo üî®\n",
    "\n",
    "\n",
    "üìä Proceso:\n",
    "- Split de datos\n",
    "- Escalado\n",
    "- Entrenamiento\n",
    "- M√©tricas\n",
    "\n",
    "*Tip: Configuraci√≥n para reproducibilidad* üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def train_recommended_model(data: pd.DataFrame, \n",
    "                          target_column: str,\n",
    "                          recommendation: dict,\n",
    "                          random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Entrena el modelo recomendado por GPT.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame con los datos\n",
    "        target_column: Nombre de la columna objetivo\n",
    "        recommendation: Diccionario con la recomendaci√≥n de GPT\n",
    "        random_state: Semilla para reproducibilidad\n",
    "    \n",
    "    Returns:\n",
    "        dict con el modelo entrenado y sus m√©tricas\n",
    "    \"\"\"\n",
    "    # Separar features y target\n",
    "    X = data.drop(target_column, axis=1)\n",
    "    y = data[target_column]\n",
    "    \n",
    "    # Split train-test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Escalar datos\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Seleccionar y entrenar modelo seg√∫n recomendaci√≥n\n",
    "    model_name = recommendation['best_model']\n",
    "    \n",
    "    if model_name == 'Random Forest':\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            random_state=random_state,\n",
    "            max_depth=15,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    elif model_name == 'Linear Regression':\n",
    "        model = LinearRegression()\n",
    "    elif model_name == 'XGBoost':\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Modelo no reconocido: {model_name}\")\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Hacer predicciones\n",
    "    train_pred = model.predict(X_train_scaled)\n",
    "    test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    results = {\n",
    "        'model': model,\n",
    "        'model_name': model_name,\n",
    "        'scaler': scaler,\n",
    "        'metrics': {\n",
    "            'train_r2': r2_score(y_train, train_pred),\n",
    "            'test_r2': r2_score(y_test, test_pred),\n",
    "            'train_mae': mean_squared_error(y_train, train_pred, squared=False),\n",
    "            'test_mae': mean_squared_error(y_test, test_pred, squared=False)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Evaluaci√≥n del Modelo üìä\n",
    "\n",
    "üîç Observaciones:\n",
    "- Buen R¬≤ en train \n",
    "- Diferencia train-test aceptable\n",
    "- MAE indica precisi√≥n razonable\n",
    "\n",
    "*Tip: Vigilar se√±ales de overfitting* ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_model=train_recommended_model(data, 'MedHouseVal', get_model_recommendation(data))\n",
    "results_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusiones y Pr√≥ximos Pasos üéØ\n",
    "\n",
    "üîë Puntos clave:\n",
    "- Performance del modelo\n",
    "- √Åreas de mejora\n",
    "- Pr√≥ximos pasos\n",
    "\n",
    "*Tip: GPT ofrece perspectivas adicionales para optimizaci√≥n* üìà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_performance_analysis(data_stats, outlier_info, model_results):\n",
    "    prompt = f\"\"\"\n",
    "    Como Data Scientist experto, analiza los siguientes resultados del modelo Random Forest y proporciona recomendaciones espec√≠ficas sobre el manejo de outliers:\n",
    "\n",
    "    1. M√âTRICAS ACTUALES DEL MODELO:\n",
    "    -------------------------------\n",
    "    - R¬≤ Entrenamiento: {model_results['metrics']['train_r2']:.4f}\n",
    "    - R¬≤ Test: {model_results['metrics']['test_r2']:.4f}\n",
    "    - MAE Entrenamiento: {model_results['metrics']['train_mae']:.4f}\n",
    "    - MAE Test: {model_results['metrics']['test_mae']:.4f}\n",
    "    \n",
    "    Configuraci√≥n del modelo:\n",
    "    - Tipo: Random Forest Regressor\n",
    "    - Max Depth: 15\n",
    "    - Escalador: StandardScaler\n",
    "\n",
    "    2. AN√ÅLISIS SOLICITADO:\n",
    "    ----------------------\n",
    "    A. EVALUACI√ìN DE OVERFITTING:\n",
    "    - Analiza la diferencia entre R¬≤ de entrenamiento ({model_results['metrics']['train_r2']:.4f}) y test ({model_results['metrics']['test_r2']:.4f})\n",
    "    - Eval√∫a la diferencia entre MAE de entrenamiento ({model_results['metrics']['train_mae']:.4f}) y test ({model_results['metrics']['test_mae']:.4f})\n",
    "    - ¬øQu√© indican estas diferencias sobre el comportamiento del modelo?\n",
    "\n",
    "    B. IMPACTO DE OUTLIERS:\n",
    "    - Considerando las m√©tricas actuales, ¬øc√≥mo podr√≠an estar afectando los outliers?\n",
    "    - ¬øLa diferencia en MAE sugiere problemas con valores extremos?\n",
    "    - An√°lisis de los outliers identificados:\n",
    "    {outlier_info}\n",
    "\n",
    "    C. RECOMENDACIONES ESPEC√çFICAS:\n",
    "    1. Estrategias para mejorar el R¬≤ de test sin comprometer la generalizaci√≥n\n",
    "    2. T√©cnicas para reducir el MAE, especialmente en el conjunto de test\n",
    "    3. Ajustes sugeridos en los hiperpar√°metros del Random Forest\n",
    "    4. Recomendaciones sobre el uso del StandardScaler con los outliers\n",
    "\n",
    "    D. PLAN DE ACCI√ìN PRIORIZADO:\n",
    "    1. Cambios inmediatos para mejorar el rendimiento\n",
    "    2. Experimentos sugeridos para validar el impacto de los cambios\n",
    "    3. M√©tricas adicionales a monitorear\n",
    "    4. Valores objetivo realistas para cada m√©trica\n",
    "\n",
    "    E. VALIDACI√ìN DE RESULTADOS:\n",
    "    - Suggestions para cross-validation considerando los outliers\n",
    "    - M√©tricas adicionales relevantes para este caso\n",
    "    - Umbrales de aceptaci√≥n para cada m√©trica\n",
    "\n",
    "    Por favor, proporciona recomendaciones concretas y accionables, priorizando las que tengan mayor impacto en reducir la brecha entre las m√©tricas de entrenamiento y test.\n",
    "    \"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Ejemplo de uso\n",
    "conclusiones_model = gpt_query(\n",
    "    get_model_performance_analysis(\n",
    "        data_stats=data.describe().to_string(),\n",
    "        outlier_info=outlier_df.to_string(),\n",
    "        model_results=results_model\n",
    "    )\n",
    ")\n",
    "\n",
    "console.print(Markdown(conclusiones_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
